#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os, sys, array, argparse, logging, time, datetime

startTime = time.time()
app = os.path.basename(__file__)
max_window = 1000000

# Красивый хелп
class AParser(argparse.ArgumentParser) :
  def help_message(name = None) :
    print ''
    print 'Usage:'
    print '  ./' + app + ' -t [input bam-file] [Optional arguments]'
    print ''
    print 'Optional arguments:'
    print '  -t,  --input      Path to `input.bam` file'
    print '  -c,  --control    Path to `control.bam` file.  DEFAULT: no control file'
    print '  -w,  --window     Window size (bp).  DEFAULT: 200'
    print '  -e,  --gms        Proportion of effective genome length; has to be in [0.0, 1.0]  DEFAULT: auto'
    print '  -g,  --gap        Gap size shows how many bases could be skipped  DEFAULT: 200'
    print '  -p,  --pvalue     P-value; has to be in [0.0, 1.0]  DEFAULT: 0.1'
    print '  -x,  --threshold  Island score threshold  DEFAULT: 0'
    print ''
    print 'Examples:'
    print '  ./' + app + ' input.bam'
    print '  ./' + app + ' -t input.bam -c control.bam -w 100 -g 100 -e 0.845'
    print ''
  def error(self, message):
    sys.stderr.write('Error:\n  %s\n' % message)
    self.help_message()
    sys.exit(2)

# ---------------------------------------------------------------------------- #

parser = AParser()
parser.add_argument('-t', '--track',     type = argparse.FileType('r'))
parser.add_argument('-o', '--output',    type = argparse.FileType('w'))
parser.add_argument('-c', '--control',   default = None, type = argparse.FileType('r'))
parser.add_argument('-w', '--window',    default = 200, type = int)
parser.add_argument('-e', '--gms',       default = 'auto')
parser.add_argument('-g', '--gap',       default = 200, type = int)
parser.add_argument('-p', '--pvalue',    default = 0.1, type = float)
parser.add_argument('-x', '--threshold', default = 0, type = float)

if len(sys.argv) == 1 :
  parser.help_message()
  sys.exit(2)

if len(sys.argv) == 2 :
  args = parser.parse_args(['-t', sys.argv[1]])
else :
  args = parser.parse_args()

if args.track.name[-4:] == '.bam' :
  logfile = args.track.name[0:-4] + '_output.log'
  resultf = args.track.name[0:-4] + '_peaks.bed'
else :
  logfile = args.track.name + '_output.log'
  resultf = args.track.name + '_peaks.bed'
if args.output.name :
  resultf = args.output.name #  + '_peaks.bed'

logging.basicConfig(filename = logfile, level = logging.DEBUG, format = '%(message)s')
logging.info('=' * 80)
logging.info('Date: ' + datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d %H:%M:%S'))
logging.info('Command:')

cmd = ['--track ' + str(args.track.name)]
if args.control : cmd.append('--control ' + str(args.control.name))
if args.output : cmd.append('--output ' + str(args.output.name))

cmd.extend([
  '--window ' + str(args.window),
  '--gms ' + str(args.gms),
  '--gap ' + str(args.gap),
  '--pvalue ' + str(args.pvalue),
  '--threshold ' + str(args.threshold)
])
logging.info(sys.argv[0] + ' \\\n  ' + (' \\\n  ').join(cmd))
logging.info('-' * 80)
# Source:
# https://github.com/JohnLonginotto/pybam

# import os
# import sys
import zlib
import struct
import itertools
import subprocess

##########################################
## Documentation ? What documentation.. ##
#########################################################################################################################################################
##                                                                                                                                                     ##
## Thank you for taking an interest in this program!                                                                                                   ##
## Note that this is really a proof-of-concept rather than a fully-fledged alternative to any of the existing BAM -> Python shared modules like        ##
## pysam and htspython. I just wanted to show that pure python is often as-fast-or-faster than shared C librarys, particularly if you're running pypy. ##
## This is because every time python has to go out of the VM and talk to external things (like htslib), we take a huge performance hit. Its the        ##
## program equivilent of walking to your car to go to the shops, when you might be able to cycle straight there in half the time.                      ##
##                                                                                                                                                     ##
## Particularly if your bicycle has rockets on it because you installed pypy.                                                                          ##
##                                                                                                                                                     ##
## If you would like to contribute to the project in anyway, please do. Better support for tags is greatly needed (ideally we need to find some data   ##
## with some weird tag formats so we can figure out how to parse them), as well as more time spent improving the interface. If you dont like the name  ##
## of something, or the way something looks/feels when you use pybam (or even the name pybam) we can change that :)                                    ##
##                                                                                                                                                     ##
#########################################################################################################################################################

'''
Awesome people who have directly contributed to the project so far:

'''

#############
## bgunzip ##
#########################################################################################################################################################
##                                                                                                                                                     ##
## There are only two parts to pybam. The first is the bgunzip class that will take either pure BAM or bgzip'd BAM data, decompress the blocks to pure ##
## BAM, parse out the BAM header, and from then on act like a generator - where every time you iterate it, it give you a random chunk of BAM data,     ##
## starting from the first read in the file (but blocks end randomly due to the way bgzip was originally designed to be data agnostic).                ##
## So you initialize with a file handle or path, and an optional blocks_at_a_time parameter (how many bgzip blocks to decompress and give you back per ##
## iteration), like so:                                                                                                                                ##
##                                                                                                                                                     ##
## >>>> pure_bam_data = pybam.bgunzip('./ENCFF001LCU.bam.gz')                                                                                          ##
## >>>> pure_bam_data.chromosome_names                                                                                                                 ##
## ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17' ... ##
## >>>> pure_bam_data.chromosome_lengths                                                                                                               ##
## [197195432, 181748087, 159599783, 155630120, 152537259, 149517037, 152524553, 131738871, 124076172, 129993255, 121843856, 121257530, 120284312, ... ##
## >>>> pure_bam_data.chromosomes_from_header                                                                                                          ##
## ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17' ... ##
## >>>> pure_bam_data.header_text                                                                                                                      ##
## '@SQ\tSN:chr1\tLN:197195432\tAS:mm9\tSP:mouse\n@SQ\tSN:chr2\tLN:181748087\tAS:mm9\tSP:mouse\n@SQ\tSN:chr3\tLN:159599783\tAS:mm9\tSP:mouse\n@SQ\ ... ##
## >>>> pure_bam_data.original_binary_header                                                                                                           ##
## <read-only buffer for 0x00007fbe9c1344e0, size 1161>                                                                                                ##
##                                                                                                                                                     ##
## You can use the original_binary_header to make a new BAM file, if that's your thing, but I wouldn't recommend it. Use the bam+ format instead.      ##
##                                                                                                                                                     ##
## Now you can iterate it like any other generator:                                                                                                    ##
## >>>> first_50_blocks = next(pure_bam_data)    # 50 because that is the blocks_at_a_time default, or about 3Mb of BAM data                           ##
## >>>> len(first_50_blocks)                                                                                                                           ##
## 3275639                                                                                                                                             ##
## >>>> first_50_blocks[:70]                                                                                                                           ##
## '\x92\x00\x00\x00\x00\x00\x00\x00\xa6\xc9-\x00 \x00\x00\x13\x01\x00\x00\x00$\x00\x00\x00\xff\xff\xff\xff\xff\xff\xff\xff\x00\x00\x00\x00SOLEXA1 ... ##
##                                                                                                                                                     ##
## What a time to be alive.                                                                                                                            ##
##                                                                                                                                                     ##
## Truly, none of this could have been done without the already fantastic work by Peter Cock, author of the bgzip format and bgzf.py from Biopython.   ##
## I'm sure he wouldn't mind me posting a link to his fantastic Bioinformatics blog, particularly this  post about bgzip and how it works for any kind ##
## of data, not just BAM data: http://blastedbio.blogspot.de/2011/11/bgzf-blocked-bigger-better-gzip.html                                              ##
## Although I didn't actually use any of Peter's code directly, I didn't need to, because he documents his code SO WELL in bgzf.py that I "got it" on  ##
## the first read-through, and could re-impliment it optimized for PyPy which is what you see here below.                                              ##
#########################################################################################################################################################
class bgunzip:
    def __init__(self,file_handle,blocks_at_a_time=50):
        ## First we init some basic things so that we will fill up in a sec.
        self.blocks_at_a_time = blocks_at_a_time
        self.bytes_read = 0
        self.rows = 0
        self.chromosome_names = []
        self.chromosome_lengths = []
        self.chromosomes_from_header = []
        self.file_handle = file_handle

        # Now we init the generator that, when iterated, will return some chunk of uncompress data.
        self.generator = self._generator()

        # We grab the first chunk, which is more than enough to contain all the header information, and parse the binary header:
        first_chunk = next(self.generator)
        if first_chunk[:4] != 'BAM\1': print 'ERROR: This doesnt look like a bam file to me :/'; exit()
        length_of_header = struct.unpack('<i',first_chunk[4:8])[0]
        self.header_text = struct.unpack(str(length_of_header)+'s',first_chunk[8:8+length_of_header])[0]
        self.number_of_reference_sequences = struct.unpack('<i',first_chunk[8+length_of_header:12+length_of_header])[0]
        rs = length_of_header + 12
        for _ in range(self.number_of_reference_sequences):
            l_name = struct.unpack('<l',first_chunk[rs:rs+4])[0]
            name, l_ref = struct.unpack('<%dsl' % l_name, first_chunk[rs+4:rs+l_name+8]) # disgusting.
            self.chromosome_names.append(name[:-1]) # We dont need the NUL byte.
            self.chromosome_lengths.append(l_ref)
            rs += 8 + l_name
        self.original_binary_header = buffer(first_chunk[:rs])
        self.left_overs = first_chunk[rs:] # bgzip doesnt care about where it drops the division between blocks, so we end up with some
                                           # bytes in the first block that contain read data, not header data.
        for line in self.header_text.split('\n'):
            if line.startswith('@SQ\tSN:'):
                self.chromosomes_from_header.append(line.split('\t')[1][3:]) # God I hope no programs put spaces in the headers instead of tabs..
        if self.chromosomes_from_header != self.chromosome_names:
            print 'ERROR: For some reason, and I have no idea why, the BAM file stores the chromosome name in two locations, the '
            print '       ASCII text header we all know and love, viewable with samtools view -H, and another special binary header'
            print '       which is used to translate the chromosome refID (a number) into a chromosome RNAME when you do bam -> sam.'
            print '       These two headers should always be the same, but apparently someone has messed that up. #YOLO\n'
            print 'Your ASCII header looks like:\n' + self.header_text
            print '\nWhile your binary header has the following chromosomes:'
            print self.chromosome_names
            exit()

        # Now we have the header data stored, but our first call to the generator gave us a block of uncompressed data back that contained way more than
        # just the header data alone. We want this class to be a generator that, on every request, hands back READ data in BAM format uncompressed, starting
        # from the first read, so in order to do that, we need to create a new generator-shim that just regurgitates what was left over from the header parse
        # the first time its called, then uses whatever self.generator would give on all subsequent tries after that.
        # This is how we do that:
        self._iterator = itertools.chain([self.left_overs],self.generator)

    # And this tells python the class is a generator:
    def __iter__(self): return self 
    def next(self): return next(self._iterator)

    def _generator(self):
        try:
            if type(self.file_handle) == str: p = subprocess.Popen(['pigz','-dc',self.file_handle], stdout=subprocess.PIPE)
            elif type(self.file_handle) == file: p = subprocess.Popen(['pigz','-dc'],stdin=self.file_handle, stdout=subprocess.PIPE)
            else: print 'ERROR: I do not know how to open and read from "' + str(self.file_handle) + '"'; exit()
            self.file_handle = p.stdout
            #sys.stderr.write('Using pigz!\n')
        except OSError:
            try:
                if type(self.file_handle) == str:    p = subprocess.Popen(['gzip','--stdout','--decompress','--force',self.file_handle]       , stdout=subprocess.PIPE)
                elif type(self.file_handle) == file: p = subprocess.Popen(['gzip','--stdout','--decompress','--force'], stdin=self.file_handle, stdout=subprocess.PIPE)
                else: print 'ERROR: I do not know how to open and read from "' + str(self.file_handle) + '"'; exit()
                self.file_handle = p.stdout
                #sys.stderr.write('Using gzip!\n')
            except OSError:
                sys.stderr.write('Using internal Python...\n') # We will end up using the python code below. It is faster than the gzip module, but
                                                               # due to how slow Python's zlib module is, it will end up being about 2x slower than pysam.
        data = self.file_handle.read(655360)
        self.bytes_read += 655360
        cache = []
        blocks_left_to_grab = self.blocks_at_a_time
        bs = 0
        checkpoint = 0
        #pool = Pool(processes=3) 
        #def decompress(data): return zlib.decompress(data, 47) # 47 == zlib.MAX_WBITS|32
        decompress = zlib.decompress
        while data:
            if len(data) - bs < 65536:
                data = data[bs:] + self.file_handle.read(35536)
                self.bytes_read += len(data) - bs
                bs = 0

            magic = data[bs:bs+4]
            if not magic: break # a child's heart
            if magic != "\x1f\x8b\x08\x04":
                if magic == 'BAM\1':
                    # The user has passed us already unzipped data, or we're reading from pigz/gzip :)
                    while data:
                        yield data
                        data = self.file_handle.read(35536)
                        self.bytes_read += len(data)
                    raise StopIteration
                elif magic == 'SQLi': print 'OOPS: You have used an SQLite database as your input BAM file!!'; exit()
                else:                 print 'ERROR: The input file is not in a format I understand :('       ; exit()

            try:
                # The gzip format allows compression containers to store metadata about whats inside them. bgzip uses this
                # to encode the virtual file pointers, final decompressed block size, crc checks etc - however we really dont
                # care -- we just want to unzip everything as quickly as possible. So instead of 'following the rules', and parsing this metadata safely,
                # we try to take a short-cut and jump right to the good stuff, and only if that fails we go the long-way-around and parse every bit of metadata properly:

                #cache.append(decompress(data[bs+18:more_bs-8])) ## originally i stored uncompressed data in a list and used map() to decompress in multiple threads. Was not faster.
                #new_zipped_data = data[bs:more_bs]              ## originally i decompressed the data with a standard zlib.decompress(data,47), headers and all. Was slower.
                more_bs = bs + struct.unpack("<H", data[bs+16:bs+18])[0] +1
                cache.append(decompress(data[bs+18:more_bs-8],-15))
                bs = more_bs
            except: ## zlib doesnt have a nice exception for when things go wrong. just "error"
                sys.stderr.write('INFO: Odd bzgip block detected! The author of pybam didnt think this would ever happen... please could you let me know?')
                header_data = magic + data[bs+4:bs+12]
                header_size = 12
                extra_len = struct.unpack("<H", header_data[-2:])[0]
                while header_size-12 < extra_len:
                    header_data += data[bs+12:bs+16]
                    subfield_id = header_data[-4:-2]
                    subfield_len = struct.unpack("<H", header_data[-2:])[0]
                    subfield_data = data[bs+16:bs+16+subfield_len]
                    header_data += subfield_data
                    header_size += subfield_len + 4
                    if subfield_id == 'BC': block_size = struct.unpack("<H", subfield_data)[0]
                raw_data = data[bs+16+subfield_len:bs+16+subfield_len+block_size-extra_len-19]
                crc_data = data[bs+16+subfield_len+block_size-extra_len-19:bs+16+subfield_len+block_size-extra_len-19+8] # I have left the numbers in verbose, because the above try is the optimised code.
                bs = bs+16+subfield_len+block_size-extra_len-19+8
                zipped_data = header_data + raw_data + crc_data
                cache.append(decompress(zipped_data,47)) # 31 works the same as 47.
                # Although the following in the bgzip code from biopython, its not needed if you let zlib decompress the whole zipped_data, header and crc, because it checks anyway (in C land)
                # I've left the manual crc checks in for documentation purposes:
                expected_crc = crc_data[:4]
                expected_size = struct.unpack("<I", crc_data[4:])[0]
                if len(unzipped_data) != expected_size: print 'ERROR: Failed to unpack due to a Type 1 CRC error. Could the BAM be corrupted?'; exit()
                crc = zlib.crc32(unzipped_data)
                if crc < 0: crc = struct.pack("<i", crc)
                else:       crc = struct.pack("<I", crc)
                if expected_crc != crc: print 'ERROR: Failed to unpack due to a Type 2 CRC error. Could the BAM be corrupted?'; exit()

            blocks_left_to_grab -= 1
            if blocks_left_to_grab == 0:
                yield ''.join(cache)
                cache = []
                blocks_left_to_grab = self.blocks_at_a_time
        self.file_handle.close()
        if cache != '': yield ''.join(cache)



####################
## compile_parser ##
#########################################################################################################################################################
##                                                                                                                                                     ##
## The second part of pybam is the compile_parser function, which returns a new runtime-generated function, which is what will actually parse you BAM. ##
## We generate this code on-the-fly because doing so is much more efficient that having a whole bunch of "if x: else y" all over the place.            ##
## Actually, the whole point of PyPy is to do exactly this sort of optimisation for you - it figures out that certain paths in your code never happen, ##
## and optimises the code accordingly. However, pybam obviously needs to work on regular python too, so thats why i've done it "manually" here below.  ##
## Some people take offence to the existence of exec(). These people also really really like PEP8, apart from maybe the second paragraph. meh.         ##
##                                                                                                                                                     ##
## You use it like this:                                                                                                                               ##
##                                                                                                                                                     ##
## >>>> pure_bam_data = pybam.bgunzip('./ENCFF001LCU.bam.gz')                                                                                          ##
## >>>>                                                                                                                                                ##
## >>>> parser = pybam.compile_parser(['pos','mapq','qname'])                                                                                          ##
## >>>>                                                                                                                                                ##
## >>>> for read in parser(pure_bam_data):                                                                                                             ##
## ....     print read                                                                                                                                 ##
## ....     break                                                                                                                                      ##
## ....                                                                                                                                                ##
## (3000742, 0, 'SOLEXA1_0001:4:49:11382:21230#0')                                                                                                     ##
##                                                                                                                                                     ##
## So we tell compile_parser how to make our function (that we called parser, but we could have called it anything) by passing it a list of special    ##
## strings. We can see what the function compile_parser came up with by looking at "pybam.code" after compile_paser has run:                           ##
##                                                                                                                                                     ##
## >>>> print pybam.code                                                                                                                               ##
##                                                                                                                                                     ##
## def parser(data_generator):                                                                                                                         ##
##     chunk = next(data_generator)                                                                                                                    ##
##     CtoPy = { 'A':'<c', 'c':'<b', 'C':'<B', 's':'<h', 'S':'<H', 'i':'<i', 'I':'<I' }                                                                ##
##     py4py = { 'A':  1,  'c':  1,  'C':  1,  's':  2,  'S':  2,  'i':  4 , 'I':  4  }                                                                ##
##     dna = '=ACMGRSVTWYHKDBN'                                                                                                                        ##
##     cigar_codes = 'MIDNSHP=X'                                                                                                                       ##
##     from array import array                                                                                                                         ##
##     from struct import unpack                                                                                                                       ##
##     p = 0                                                                                                                                           ##
##     while True:                                                                                                                                     ##
##         try:                                                                                                                                        ##
##             while len(chunk) < p+36: chunk = chunk[p:] + next(data_generator); p = 0                                                                ##
##             block_size,refID,pos,l_read_name,mapq                                                     = unpack('<iiiBB'       ,chunk[p:p+14])       ##
##             while len(chunk) < p + 4 + block_size: chunk = chunk[p:] + next(data_generator); p = 0                                                  ##
##         except StopIteration: break                                                                                                                 ##
##         end = p + block_size + 4                                                                                                                    ##
##         p += 36                                                                                                                                     ##
##         qname = chunk[p:p+l_read_name-1]                                                                                                            ##
##         p = end                                                                                                                                     ##
##         yield pos,mapq,qname                                                                                                                        ##
##                                                                                                                                                     ##
##                                                                                                                                                     ##
## A'int that complicated eh. So now you're probably wondering what special strings exist. Well, for now thats only a few. Many more could be added to ##
## do more abstract things, but right now we've got:                                                                                                   ##
##                                                                                                                                                     ##
#########################################################################################################################################################

def compile_parser(fields):
    deps = set(fields)
    unpack = '''
def parser(data_generator):
    chunk = next(data_generator)
    CtoPy = { 'A':'<c', 'c':'<b', 'C':'<B', 's':'<h', 'S':'<H', 'i':'<i', 'I':'<I' }
    py4py = { 'A':  1,  'c':  1,  'C':  1,  's':  2,  'S':  2,  'i':  4 , 'I':  4  }
    dna = '=ACMGRSVTWYHKDBN'
    cigar_codes = 'MIDNSHP=X'
    from array import array
    from struct import unpack
    p = 0
    while True:
        try:
            while len(chunk) < p+36: chunk = chunk[p:] + next(data_generator); p = 0

            '''

    # Some values require the length of the previous values to be known before they can be grabbed. Add those to our 'dependency' list:
    if  'tags' in deps or 'tags_bam'  in deps: deps.update(['qual_skip','seq_skip','l_seq_bytes','l_seq','cigar_skip','n_cigar_op','qname_skip','l_read_name','fixed_skip'])
    if  'qual' in deps or 'qual_bam'  in deps: deps.update([            'seq_skip','l_seq_bytes','l_seq','cigar_skip','n_cigar_op','qname_skip','l_read_name','fixed_skip'])
    if   'seq' in deps or 'seq_bam'   in deps: deps.update([                       'l_seq_bytes','l_seq','cigar_skip','n_cigar_op','qname_skip','l_read_name','fixed_skip'])
    if 'cigar' in deps or 'cigar_bam' in deps: deps.update([                                                          'n_cigar_op','qname_skip','l_read_name','fixed_skip'])
    if 'qname' in deps or 'qname_bam' in deps: deps.update([                                                                                    'l_read_name','fixed_skip'])

    # Fixed length SAM data. Because its often quicker to unpack more data in a single stuct.unpack call than it is to unpack less data with 2 or more calls to unpack, the logic here
    # is very simple - we unpack everything up until the value desired. It needs to be tested if this is actually holds true in real-world data though.
    if   'tlen'        in deps: unpack += "block_size,refID,pos,l_read_name,mapq,bin_,n_cigar_op,flag,l_seq,next_refID,next_pos,tlen = unpack('<iiiBBHHHiiii',chunk[p:p+36])"    ; fixed_length = 36
    elif 'next_pos'    in deps: unpack += "block_size,refID,pos,l_read_name,mapq,bin_,n_cigar_op,flag,l_seq,next_refID,next_pos      = unpack('<iiiBBHHHiii' ,chunk[p:p+32])"    ; fixed_length = 32
    elif 'next_refID'  in deps: unpack += "block_size,refID,pos,l_read_name,mapq,bin_,n_cigar_op,flag,l_seq,next_refID               = unpack('<iiiBBHHHii'  ,chunk[p:p+28])"    ; fixed_length = 28
    elif 'l_seq'       in deps: unpack += "block_size,refID,pos,l_read_name,mapq,bin_,n_cigar_op,flag,l_seq                          = unpack('<iiiBBHHHi'   ,chunk[p:p+24])"    ; fixed_length = 24
    elif 'flag'        in deps: unpack += "block_size,refID,pos,l_read_name,mapq,bin_,n_cigar_op,flag                                = unpack('<iiiBBHHH'    ,chunk[p:p+20])"    ; fixed_length = 20
    elif 'n_cigar_op'  in deps: unpack += "block_size,refID,pos,l_read_name,mapq,bin_,n_cigar_op                                     = unpack('<iiiBBHH'     ,chunk[p:p+18])"    ; fixed_length = 18
    elif 'bin'         in deps: unpack += "block_size,refID,pos,l_read_name,mapq,bin_                                                = unpack('<iiiBBH'      ,chunk[p:p+16])"    ; fixed_length = 16
    elif 'mapq'        in deps: unpack += "block_size,refID,pos,l_read_name,mapq                                                     = unpack('<iiiBB'       ,chunk[p:p+14])"    ; fixed_length = 14
    elif 'l_read_name' in deps: unpack += "block_size,refID,pos,l_read_name                                                          = unpack('<iiiB'        ,chunk[p:p+13])"    ; fixed_length = 13
    elif 'pos'         in deps: unpack += "block_size,refID,pos                                                                      = unpack('<iii'         ,chunk[p:p+12])"    ; fixed_length = 12
    elif 'refID'       in deps: unpack += "block_size,refID                                                                          = unpack('<ii'          ,chunk[p:p+8 ])"    ; fixed_length = 8
    else:                       unpack += "block_size                                                                                = unpack('<i'           ,chunk[p:p+4 ])[0]" ; fixed_length = 4

    # Fixed-length BAM data (where we just grab the bytes, we dont unpack) can, however, be grabbed individually.
    if 'fixed_bam'       in deps: unpack += "\n            fixed_bam       = chunk[p:p+36]"    # All the fixed data.
    if 'block_size_bam'  in deps: unpack += "\n            block_size_bam  = chunk[p:p+4]"
    if 'refID_bam'       in deps: unpack += "\n            refID_bam       = chunk[p+4:p+8]"
    if 'pos_bam'         in deps: unpack += "\n            pos_bam         = chunk[p+8:p+12]"
    if 'l_read_name_bam' in deps: unpack += "\n            l_read_name_bam = chunk[p+12:p+13]"
    if 'mapq_bam'        in deps: unpack += "\n            mapq_bam        = chunk[p+13:p+14]"
    if 'bin_bam'         in deps: unpack += "\n            bin_bam         = chunk[p+14:p+16]"
    if 'n_cigar_op_bam'  in deps: unpack += "\n            n_cigar_op_bam  = chunk[p+16:p+18]"
    if 'flag_bam'        in deps: unpack += "\n            flag_bam        = chunk[p+18:p+20]"
    if 'l_seq_bam'       in deps: unpack += "\n            l_seq_bam       = chunk[p+20:p+24]"
    if 'next_refID_bam'  in deps: unpack += "\n            next_refID_bam  = chunk[p+24:p+28]"
    if 'next_pos_bam'    in deps: unpack += "\n            next_pos_bam    = chunk[p+28:p+32]"
    if 'tlen_bam'        in deps: unpack += "\n            tlen_bam        = chunk[p+32:p+36]"

    unpack += '''
            while len(chunk) < p + 4 + block_size: chunk = chunk[p:] + next(data_generator); p = 0
        except StopIteration: break
        end = p + block_size + 4
'''

    if 'variable_bam'in deps: unpack += "        variable_bam = chunk[p:end]\n"
    if 'fixed_skip'  in deps: unpack += "        p += 36\n"
    if 'qname_bam'   in deps: unpack += "        qname_bam = chunk[p:p+l_read_name]\n"
    if 'qname'       in deps: unpack += "        qname = chunk[p:p+l_read_name-1]\n"
    if 'qname_skip'  in deps: unpack += "        p += l_read_name\n"
    if 'cigar'       in deps: unpack += "        cigar = [(cigar_codes[cig & 0b1111],cig>>4) for cig in array('I',chunk[p:p+(4*n_cigar_op)])]\n"
    if 'cigar_bam'   in deps: unpack += "        cigar_bam = chunk[p:p+(4*n_cigar_op)]\n"
    if 'cigar_skip'  in deps: unpack += "        p += n_cigar_op*4\n"
    if 'l_seq_bytes' in deps: unpack += "        l_seq_bytes = -((-l_seq)//2)\n"
    if 'seq'         in deps: unpack += "        seq = ''.join([ dna[bit4 >> 4] + dna[bit4 & 0b1111] for bit4 in array('B', chunk[p:p+l_seq_bytes]) ])\n"
    if 'seq_bam'     in deps: unpack += "        seq_bam = chunk[p:p+l_seq_bytes]\n"
    if 'seq_skip'    in deps: unpack += "        p += l_seq_bytes\n"
    if 'qual'        in deps: unpack += "        qual = chunk[p:p+l_seq]\n"
    if 'qual_bam'    in deps: unpack += "        qual_bam = chunk[p:p+l_seq]\n"
    if 'qual_skip'   in deps: unpack += "        p += l_seq\n"
    if 'tags_bam'    in deps: unpack += "        tags_bam = chunk[p:end]\n"
    if 'tags'        in deps: unpack += '''
        tags = []
        while p != end:
            tag_name = chunk[p:p+2]
            tag_type = chunk[p+2]
            if tag_type == 'Z':
                p_end = chunk.index('\\0',p+3)+1
                tag_data = chunk[p+3:p_end];  # I've opted to keep the NUL byte in the output, although it makes me feel dirty.
            elif tag_type in CtoPy:
                p_end = p+3+py4py[tag_type]
                tag_data = unpack(CtoPy[tag_type],chunk[p+3:p_end])[0]
            else:
                print 'PROGRAMMER ERROR: I dont know how to parse BAM tags in this format: ',repr(tag_type)
                print '                  This is simply because I never saw this kind of tag during development.'
                print '                  If you could mail the following chunk of text to john at john.uk.com, ill fix this up :)'
                print repr(tag_type),repr(chunk[p+3:end])
                exit()
            tags.append((tag_name,tag_type,tag_data))
            p = p_end\n'''
    unpack += '        p = end\n'
    unpack += '        yield ' + ','.join([x for x in fields])
    global code    # To allow user to view
    code = unpack  # code with pybam.code

    exec(unpack)  # "parser" now comes into existance
    return parser

#######################
## THE END / CREDITS ##
#########################################################################################################################################################
##                                                                                                                                                     ##
## Thats it!                                                                                                                                           ##
## This code was written by John Longinotto, a PhD student of the Pospisilik Lab at the Max Planck Institute of Immunbiology & Epigenetics, Freiburg.  ##
## My PhD is funded by the Deutsches Epigenom Programm (DEEP), and the Max Planck IMPRS Program.                                                       ##
## I study Adipose Biology and Circadian Rhythm in mice, although it seems these days I spend most of my time at the computer and not at the bench.    ##
##                                                                                                                                                     ##
#########################################################################################################################################################
# ---------------------------------------------------------------------------- #
species = {
  'anoGam1': [62725911,59568033,53272125,48795086,41284009,22145176,15363],
  'apiMel2': [321143811,25433307,14233499,14191905,13312070,12413357,11790680,10993160,10665743,9968276,9828392,9757938,8966624,8846036,8017760,7326077,5608962],
  'braFlo1': [926371504,15083],
  'caePb2': [194283334],
  'cb3': [20608032,16004101,15290274,14512975,13544562,11274843,7311690,3509021,2554181,2252910,864856,751081,14420],
  'ce11': [20924180,17718942,17493829,15279421,15072434,13783801,13794],
  'caeJap1': [156378573],
  'caeRem3': [149111736],
  'dp3': [30711475,19738957,14308685,12499574,11635473,9190824,9052427,6620765,6604477,6604331,5302587,2686958,2329291,1468299,1217433,733150,374547],
  'droSim1': [27517382,22553184,22036055,19596830,17042790,15797150,5698898,3178526,2996586,1452968,1307089,1049610,949497,909653,134295,100575,84659,14972],
  'droYak2': [28832112,28119190,24197627,22324452,21770863,21139217,4797643,4064425,3774259,3277457,1802292,1374474,911842,894407,720513,261230,172766,114151,67238,31700,16019],
  'eboVir3': [18957],
  'equCab2': [185838109,124114077,120857687,119479920,117461955,108569075,99680356,98542428,94057673,93904894,91571448,87365405,84719076,83980604,83561422,82527541,80757907,64166202,61308211,59975221,57723302,55726280,49946797,46749900,46177339,42578167,41866177,39960074],
  'fr2': [400509343,16447],
  'hg19': [249250621,243199373,198022430,191154276,180915260,171115067,159138663,155270560,146364022,141213431,135534747,135006516,133851895,115169878,107349540,102531392,90354753,81195210,78077248,63025520,59373566,59128983,51304566,48129895,4928567,4833398,4795371,4683263],
  'monDom5': [748055161,541556283,527952102,435153693,312544902,304825324,292091736,260857928,103241611,79335909,17079],
  'mm10': [195471971,182113224,171031299,160039680,156508116,151834684,149736546,145441459,130694993,129401213,124902244,124595110,122082543,120421639,120129022,104043685,98207768,94987271,91744698,90702639,61431566,953012,336933,259875,241735,227966,207968,206961],
  'ponAbe2': [229942017,202140232,198332218,183952662,174210431,157549271,156195299,153482349,136387465,135191526,135000294,133410057,132107971,117095149,113028656,108868599,99152023,94050890,77800216,73212453,72422247,62736349,60714840,48394510,46535552,39000292,35968885,23176937],
  'ponAbe2': [229942017,202140232,198332218,183952662,174210431,157549271,156195299,153482349,136387465,135191526,135000294,133410057,132107971,117095149,113028656,108868599,99152023,94050890,77800216,73212453,72422247,62736349,60714840,48394510,46535552,39000292,35968885,23176937],
  'priPac1': [174852139],
  'sacCer3': [1531933,1091291,1090940,1078177,948066,924431,813184,784333,745751,666816,576874,562643,439888,316620,270161,230218,85779],
  'susScr2': [295529705,148510138,145235301,140133492,136409062,136254946,134541103,132468591,125871292,123599780,123305171,119985671,100516970,79814395,77435658,66736929,64395339,57431344,54309914,16770],
  'fr2': [400509343,16447],
  'tetNig2': [107808587,22981688,21591555,15489435,13390619,13302670,13272281,12622881,12136232,11954808,11693588,11077504,10554956,10512681,10246949,9874776,9031048,7320470,7272499,7024381,5834722,3798727,3311323,3234215,2082209,1180980,16462],
  'hg18': [247249719,242951149,199501827,191273063,180857866,170899992,158821424,154913754,146274826,140273252,135374737,134452384,132349534,114142980,106368585,100338915,88827254,78774742,76117153,63811651,62435964,57772954,49691432,46944323,4731698,4565931,2617613,1875562]
}

spnames = {
  'anoGam1':'Anopheles gambiae',
  'apiMel2':'Apis mellifera',
  'braFlo1':'Branchiostoma floridae',
  'caePb2':'Caenorhabditis brenneri',
  'cb3':'Caenorhabditis briggsae',
  'ce11':'Caenorhabditis elegans',
  'caeJap1':'Caenorhabditis japonica',
  'caeRem3':'Caenorhabditis remanei',
  'dp3':'Drosophila pseudoobscura',
  'droSim1':'Drosophila simulans',
  'droYak2':'Drosophila yakuba',
  'eboVir3':'Ebola virus',
  'equCab2':'Equus caballus',
  'fr2':'Takifugu rubripes',
  'hg19':'Homo sapiens',
  'monDom5':'Monodelphis domestica',
  'mm10':'Mus musculus',
  'ponAbe2':'Pongo pygmaeus abelii',
  'priPac1':'Pristionchus pacificus',
  'sacCer3':'Saccharomyces cerevisiae',
  'susScr2':'Sus scrofa',
  'tetNig2':'Tetraodon nigroviridis',
  'hg18':'Homo sapiens'
}

# ---------------------------------------------------------------------------- #
def define_specie(info) :
  vote = {}
  total = 1
  for c in info :
    total += info[c]['Unique reads']

  for c in info :
    dists = []
    for table in species :
      for chr in species[table]:
        dists.append([pow(chr - info[c]['Length'], 2), table])
    v = sorted(dists, key=lambda e: e[0])
    for k in [0,1,2] :
      if v[k][1] not in vote : vote[v[k][1]] = 0
      vote[v[k][1]] += float(3 - k) * info[c]['Unique reads']/total
  code = max(vote, key = vote.get)
  return [code, spnames[code]]


# ---------------------------------------------------------------------------- #
def parser(data_generator):
  chunk = next(data_generator)
  CtoPy = { 'A':'<c', 'c':'<b', 'C':'<B', 's':'<h', 'S':'<H', 'i':'<i', 'I':'<I' }
  py4py = { 'A':  1,  'c':  1,  'C':  1,  's':  2,  'S':  2,  'i':  4 , 'I':  4  }
  dna = '=ACMGRSVTWYHKDBN'
  cigar_codes = 'MIDNSHP=X'
  from array import array
  from struct import unpack
  p = 0
  while True:
    try:
      while len(chunk) < p+36: chunk = chunk[p:] + next(data_generator); p = 0
      block_size,refID,pos,l_read_name,mapq,bin_,n_cigar_op,flag,l_seq                          = unpack('<iiiBBHHHi'   ,chunk[p:p+24])
      while len(chunk) < p + 4 + block_size: chunk = chunk[p:] + next(data_generator); p = 0
    except StopIteration: break
    end = p + block_size + 4
    p = end
    yield pos,refID,flag,l_seq


# ---------------------------------------------------------------------------- #
def chrsort(s) :
  s = s.lower()  
  if s[0:3] == 'chr' : 
    s = s.replace('chr','')
    if unicode(s, 'utf-8').isnumeric() : return int(s)
    return 40
  return 999

def count_unique_reads(track, maxdr) :
  reads = { 0 : 0, 16 : 0 }
  drate = { 0 : 0, 16 : 0 }
  lhist = {}
  info = {}
  
  for read in parser(track) :
    pos, chr, strand, l_seq = read
    if chr < 0 or chr > len(track.chromosome_names) : continue
    if l_seq not in lhist : lhist[l_seq] = 0
    lhist[l_seq] += 1
    c = track.chromosome_names[chr]
    # New chromosome name:
    if c not in info : 
      info[c] = {
        'Data' : array.array('l', []), # position * 10 + strand indicator (0 or 1)
        'Length' : track.chromosome_lengths[chr],
        'Unique reads' : 0,
        'Total reads' : 0,
        'Beginning of the previous' : 0
      }
    # Два рида в одном месте
    if pos == info[c]['Beginning of the previous'] :
      drate[strand] += 1
      if drate[strand] <= maxdr :
        info[c]['Data'].append(10 * pos + (strand % 15))
        info[c]['Unique reads'] += 1
        reads[strand] += 1
    else :
      drate = { 0 : 0, 16 : 0 }
      info[c]['Beginning of the previous'] = pos
      info[c]['Data'].append(10 * pos + (strand % 15))
      info[c]['Unique reads'] += 1
      reads[strand] += 1
    info[c]['Total reads'] += 1
  # <- for

  total_reads = 0; unique_reads = 0
  logging.info("Chromosome, unique reads, total reads:")

  keys = info.keys()
  keys = sorted(keys, key = lambda (c): chrsort(c))
  for c in keys :
    total_reads  += info[c]['Total reads']
    unique_reads += info[c]['Unique reads']
    logging.info("{}\t{}\t{}".format(c, info[c]['Unique reads'], info[c]['Total reads']))
  # <- for
  
  mean_length = 0
  for l in lhist :
    mean_length += l * lhist[l]
  mean_length = mean_length/total_reads

  logging.info('-' * 80)
  
  msg = "Average read length: {}"
  logging.info(msg.format(mean_length))

  msg = "Library depth: there are {} unique reads out of {}"
  logging.info(msg.format(unique_reads, total_reads))

  msg = "Duplication rate: {}"
  logging.info(msg.format(round(1 - float(unique_reads)/float(total_reads)*100, 1)))

  msg = "Strand symmetry:\n  {} (+)\n  {} (-)"
  logging.info(msg.format(reads[0], reads[16]))

  return [unique_reads, mean_length, info]


# ---------------------------------------------------------------------------- #
def get_gms(info, length, gms) :
  if gms > 0 and gms <= 1 :
    msg = "Using custom effective proportion: {}"
    logging.info(msg.format(gms))
    return gms
  
  specie, specie_name = define_specie(info)
  GMS = {}
  GMS['hg19'] = [[20,0.697969],[25,0.750921],[30,0.782328],[40,0.824712],[60,0.867066],[80,0.882649],[100,0.889473],[120,0.893435],[140,0.896156]]
  GMS['mm10'] = [[25,0.766128],[30,0.790964],[40,0.822389],[60,0.857177],[80,0.875752],[100,0.88708],[120,0.894755],[140,0.900449]]

  # Default value
  if specie not in GMS : 
    gms = 0.77
    msg = "Using default effective proportion: {}"
    logging.info(msg.format(gms))
    return gms

  # Approximation
  if GMS[specie][0][0] > length :
    gms = GMS[specie][0][1]
  if GMS[specie][-1][0] < length :
    gms = GMS[specie][-1][1]
  
  for e in range(0, len(GMS[specie]) - 1) :
    a1, v1 = GMS[specie][e]
    a2, v2 = GMS[specie][e + 1]
    if a2 >= length and a1 <= length :
      # (a2 - a1)/(v2 - v1) == (length - a1)/(x - v1)
      gms = (length - a1) * (v2 - v1)/(a2 - a1) + v1

  # ...
  msg = "Using default effective proportion for {}: {}"
  logging.info(msg.format(specie_name, gms))
  return gms


# ---------------------------------------------------------------------------- #
def count_effective_length(effective_proportion, track) :
  return effective_proportion * sum(track.chromosome_lengths)


# ---------------------------------------------------------------------------- #
def count_lambda(unique_reads_count, wsize, effective_length):
  lambdaa = float(wsize) * float(unique_reads_count) / float(effective_length)
  msg = "Average density of reads per {} bp window is {}"
  logging.info(msg.format(wsize, round(lambdaa, 2)))
  return lambdaa


# ---------------------------------------------------------------------------- #
def make_windows_list(info, l0, window_size, gap, normalization_coef):
  msg = "Making eligible windows of {} bp with allowed gap_size {} bp"
  logging.info(msg.format(window_size, window_size * gap))
  logging.info('-' * 80)
  logging.info("Chromosome, eligible windows:")

  keys = info.keys()
  keys = sorted(keys, key = lambda (c): chrsort(c))
  for c in keys :
    info[c]['Windows'] = array.array('l', [])
    previous = 0
    previous_read_strand = 0
    gap_count = 0
    window_start = 0
    window_reads_count = 0
    chr_window = 0
    for read in info[c]['Data'] :
      read_strand = read % 10
      begin = (read - read_strand) / 10
      if (begin != previous) or (read_strand != previous_read_strand):
        previous = begin
        previous_read_strand = read_strand
        gap_flag = True
        while True:
          if window_start <= begin < window_start + window_size:
            window_reads_count += 1
            break
          elif begin < window_start:
            break
          else:
            window_reads_count = int(float(window_reads_count) * normalization_coef)
            if window_reads_count < l0:
              gap_count += 1
            else:
              gap_flag = False
              gap_count = 0
            info[c]['Windows'].append(max_window * window_start + window_reads_count)
            chr_window += 1
            if gap_count > gap or gap_flag:
              gap_flag = True
              while gap_count > 0:
                info[c]['Windows'].pop()
                gap_count -= 1
                chr_window -= 1
            window_start += window_size
            window_reads_count = 0
    if window_reads_count != 0:
      info[c]['Windows'].append(max_window * window_start + window_reads_count)
    logging.info("{}\t{}".format(c, chr_window))
  return info

def write_islands_list(info, lambdaa, wsize, l0, threshold, resultf):
  f = open(resultf, 'w+'); islands = 0; coverage = 0

  def score(reads):
    if reads >= l0:
      temp = scipy.stats.poisson.pmf(reads, lambdaa)
      if temp < 1e-320: window_score = 1000
      else: window_score = -numpy.log(temp)
    else:
      window_score = 0
    return window_score

  def write(c, e) :
    if e['score'] < threshold : return 0, 0
    f.write(c+'\t'+str(e['from'])+'\t'+str(e['to'])+'\t'+str(e['score'])+'\t'+str(e['reads'])+'\t'+str(e['count'] - e['gaps'])+'\t'+str(e['gaps'])+'\n')
    return 1, (e['to'] - e['from'])

  def new_island(init, reads):
    return { 
      'from'  : init, 
      'to'    : init + wsize, 
      'score' : score(reads), 
      'reads' : reads, 
      'count' : 1, 
      'gaps'  : 0
    }

  for c in info :
    island = False
    for w in info[c]['Windows'] :
      reads = w % max_window
      init = (w - reads)/max_window
      if not island :
        island = new_island(init, reads)
      else :
        # Same island
        if init == island['to'] :
          island['to'] += wsize
          island['count'] += 1
          island['reads'] += reads
          island['score'] += score(reads)
          if reads == 0 : island['gaps'] += 1
        # Other island
        else :
          i, cov = write(c, island)
          islands += i; coverage += cov
          island = new_island(init, reads)
    # <- end for
    if island :
      i, cov = write(c, island)
      islands += i; coverage += cov
      # islands += write(c, island)

  # <- end for
  f.close()
  return islands, coverage
import scipy, numpy, scipy.stats

# ---------------------------------------------------------------------------- #
print 'Step 1 of 3: Counting unique reads'

track = bgunzip(args.track.name)

# if args.maxdr < 1 : args.maxdr = 1
# reads, length, info = count_unique_reads(track, args.maxdr)
reads, length, info = count_unique_reads(track, 1)

control = False
if args.control != None : control = bgunzip(args.control.name)

# Effective proportion
try :
  gms = float(args.gms)
except :
  gms = 0

gms = get_gms(info, length, gms)

# Effective genome length
effective_length = count_effective_length(gms, track)
track.file_handle.close()

if args.window > max_window : args.window = max_window
if args.window < 10 : args.window = 10

# Lambda for poisson distribution
lambdaa = count_lambda(reads, args.window, effective_length)
# if control : control_lambdaa = count_lambda(reads, args.window, effective_length)


# Minimum count(reads) in a window for eligibility
# Formula (1), finding l0
pvalue = args.pvalue
if pvalue > 1 or pvalue <= 0 : pvalue = 0.1
l0 = scipy.stats.poisson.ppf(1 - pvalue, lambdaa)

msg = "Window read threshold is {} reads, " + \
      "\n  i.e. {} is minimum number of reads in window to consider" + \
      "\n  this window `eligible` with Poisson distribution p-value {}"
logging.info(msg.format(l0, l0, pvalue))

# ---------------------------------------------------------------------------- #

print 'Step 2 of 3: Making window list'

if args.window < 1 : args.window = 1
args.gap = args.gap/args.window
if args.gap < 0 : args.gap = 0

NORM_CONSTANT = 1
info = make_windows_list(info, l0, args.window, args.gap, NORM_CONSTANT)
logging.info('-' * 80)

# ---------------------------------------------------------------------------- #

print 'Step 3 of 3: Writing found islands'

if args.threshold < 0 : args.threshold = 0

# found, coverage = write_islands_list(info, lambdaa, args.window, l0, args.threshold, resultf)
found, coverage = write_islands_list(info, lambdaa, args.window, l0, args.threshold, resultf)
coverage_txt = str(coverage) + 'bp'
if coverage > 1000000 :
  coverage_txt = str(float(coverage)/1000000) + 'Mbp (' + coverage_txt + ')'
logging.info("Coverage: {}".format(coverage_txt))

msg = "There are {} islands found"
logging.info(msg.format(found))
print msg.format(found)

logging.info(('\n ').join([
 "Column names in BED file:",
 "1. Chromosome name",
 "2. Island start",
 "3. Island end",
 "4. Island score",
 "5. Number of reads in the island",
 "6. Number of eligible windows per island",
 "7. Number of gaps in the island"
]))

msg = "Finished. Elapsed time, minutes: {}"
logging.info(msg.format((time.time() - startTime) / 60))
logging.info("")


